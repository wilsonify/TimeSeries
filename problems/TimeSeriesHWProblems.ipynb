{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob 1.9\n",
    "1.9 \n",
    "\n",
    "A time series with a periodic component can be constructed from\n",
    "$$\n",
    "x_t = U_1 sin(2 \\pi \\omega_0 t) + U_2 cos(2 \\pi \\omega_0 t)\n",
    "$$\n",
    ",\n",
    "where $U_1$ and $U_2$ are independent random variables with zero means and $E(U_1^2) = E(U_2^2) = \\sigma^2$. \n",
    "the constant $\\omega_0$ determines the period or time it takes the process to make one complete cycle. Show that this series is weakly stationary with autocovariance function\n",
    "\n",
    "$\\gamma(h) = \\sigma 2 cos(2 \\pi \\omega_0 h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob 1.21\n",
    "1.21 \n",
    "\n",
    "(a) Simulate a series of n = 500 moving average observations as in Example 1.9 and compute the sample ACF, $\\hat{\\rho}(h)$, to lag 20. Compare the sample ACF you obtain to the actual ACF, $\\rho(h)$. [Recall Example 1.20.]\n",
    "\n",
    "(b) Repeat part (a) using only n = 50. \n",
    "How does changing n affect the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.9: Moving Averages and Filtering\n",
    "\n",
    "We might replace the white noise series $w_t$ by a moving average that smooths\n",
    "the series. For example, consider replacing $w_t$ in Example 1.8 by an average of its\n",
    "current value and its immediate neighbors in the past and future. That is, let\n",
    "$$\n",
    "v_t = \\frac{1}{3} \\left( w_{t−1} + w_t +  w_{t+1} \\right)\n",
    "$$ (1.1)\n",
    "\n",
    "which leads to the series shown in the lower panel of Fig. 1.8. \n",
    "\n",
    "![](https://raw.githubusercontent.com/wilsonify/TimeSeries/master/images/TSAAfig1.8.png)\n",
    "\n",
    "Inspecting the series shows a smoother version of the first series, reflecting the fact that the slower oscillations are more apparent and some of the faster oscillations are taken out. We begin to notice a similarity to the SOI in Fig. 1.5\n",
    "\n",
    "![](https://raw.githubusercontent.com/wilsonify/TimeSeries/master/images/TSAAfig1.5.png)\n",
    "\n",
    "A linear combination of values in a time series such as in eq (1.1) is referred to,\n",
    "generically, as a filtered series; hence the command filter in the following code\n",
    "for Fig. 1.8.\n",
    "\n",
    "```R\n",
    "w = rnorm(500,0,1)\n",
    "# 500 N(0,1) variates\n",
    "v = filter(w, sides=2, filter=rep(1/3,3)) # moving average\n",
    "par(mfrow=c(2,1))\n",
    "plot.ts(w, main=\"white noise\")\n",
    "plot.ts(v, ylim=c(-3,3), main=\"moving average\")\n",
    "```\n",
    "\n",
    "The speech series in Fig. 1.3 and the Recruitment series in Fig. 1.5, as well as\n",
    "some of the MRI series in Fig. 1.6, differ from the moving average series because one\n",
    "particular kind of oscillatory behavior seems to predominate, producing a sinusoidal\n",
    "type of behavior. A number of methods exist for generating series with this quasi-\n",
    "periodic behavior; we illustrate a popular one based on the autoregressive model\n",
    "considered in Chap. 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.20 Stationarity of a Moving Average\n",
    "\n",
    "The three-point moving average process of Example 1.9 is stationary because, the mean and autocovariance functions\n",
    "$μ_vt = 0$, and\n",
    "$$\n",
    "\\gamma_v(h) = \n",
    "\\begin{cases} \n",
    " \\frac{3}{9} \\sigma_w^2 & h=0, \\\\\n",
    " \\frac{2}{9} \\sigma_w^2 & h= \\pm 1, \\\\\n",
    " \\frac{1}{9} \\sigma_w^2 & h= \\pm 2, \\\\\n",
    " 0  & |h| > 2\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "are independent of time t, satisfying the conditions of Definition 1.7.\n",
    "The autocorrelation function is given by\n",
    "$$\n",
    "\\rho_v(h) =\n",
    "\\begin{cases} \n",
    "           1 & h= 0, \\\\\n",
    " \\frac{2}{3} & h= \\pm 1, \\\\\n",
    " \\frac{1}{3} & h= \\pm 2, \\\\\n",
    "          0  & |h| > 2\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "![](https://raw.githubusercontent.com/wilsonify/TimeSeries/master/images/TSAAfig1.12.png)\n",
    "\n",
    "Figure 1.12 shows a plot of the autocorrelations as a function of lag h. Note that\n",
    "the ACF is symmetric about lag zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob 2.3\n",
    "2.3 In this problem, we explore the difference between a random walk and a trend\n",
    "stationary process.\n",
    "\n",
    "(a) Generate four series that are random walk with drift of length $n = 100$\n",
    "with $\\delta = .01$ and $\\sigma_w = 1$. Call the data $x_t$ for $t = 1, \\dotso, 100$. Fit the regression $x_t = \\beta t + w_t$ using least squares. Plot the data, the true mean function (i.e., $\\mu_t = .01 t$) and the fitted line, $\\hat{x_t} = \\hat{\\beta} t$, on the same graph. Hint: The following R\n",
    "code may be useful.\n",
    "\n",
    "```R\n",
    "par(mfrow=c(2,2), mar=c(2.5,2.5,0,0)+.5, mgp=c(1.6,.6,0)) # set up\n",
    "for (i in 1:4){\n",
    "x = ts(cumsum(rnorm(100,.01,1))) # data\n",
    "regx = lm(x~0+time(x), na.action=NULL) #regression\n",
    "plot(x, ylab='Random Walk w Drift') # plots\n",
    "abline(a=0, b=.01, col=2, lty=2) # true mean (red - dashed)\n",
    "abline(regx, col=4) # fitted line (blue - solid)\n",
    "```\n",
    "\n",
    "(b) Generate four series of length n = 100 that are linear trend plus noise, say\n",
    "$y_t = .01 t + w_t$ , where t and $w_t$ are as in part (a). Fit the regression $y_t = \\beta t + w_t$ using least squares. Plot the data, the true mean function (i.e., $\\mu_t = .01 t$) and the fitted line, $\\hat{y_t} = \\hat{\\beta} t$, on the same graph. \n",
    "\n",
    "(c) Comment (what did you learn from this assignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob 2.11\n",
    "2.11 Use two different smoothing techniques described in Sect. 2.3 to estimate the\n",
    "trend in the global temperature series globtemp . Comment.\n",
    "\n",
    "### Methods from Section 2.3\n",
    "* Moving Average Smoother\n",
    "* Kernel Smoothing\n",
    "* Lowess\n",
    "* Smoothing Splines\n",
    "* Smoothing One Series as a Function of Another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob 3.6\n",
    "3.6 For the AR(2) model given by $x_t = −.9 x_{t−2} + w_t$ , find the roots of the\n",
    "autoregressive polynomial, and then sketch the ACF, $\\rho(h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob 3.9\n",
    "3.9 Generate n = 100 observations from each of the three models discussed in Problem 3.8.\n",
    "\n",
    "Compute the sample ACF for each model and compare it to the theoretical values.\n",
    "Compute the sample PACF for each of the generated series and compare the sample ACFs and PACFs with the general results given in\n",
    "table 3.1. Section 3.5\n",
    "\n",
    "## For Reference, Problem 3.8\n",
    "3.8 Verify the calculations for the autocorrelation function of an ARMA(1, 1) process given in Example 3.14. \n",
    "\n",
    "Compare the form with that of the ACF for the ARMA(1, 0) and the ARMA(0, 1) series.\n",
    "\n",
    "Plot the ACFs of the three series on the same graph for $\\phi = .6, \\theta = .9$, and comment on the diagnostic capabilities of the ACF in this case.\n",
    "\n",
    "### Table 3.1 Behavior of the ACF and PACF for ARMA models\n",
    "\n",
    "|      | AR(p)                | MA(q)               | ARMA(p,q) |\n",
    "|------|----------------------|---------------------|-----------|\n",
    "| ACF  | Tails off            | Cutsoff after lag q | Tails off |\n",
    "| PACF | Cuts off after lag p | Tails off           | Tails off |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob 3.21\n",
    "Generate 10 realizations of length $n = 200$ each of an ARMA(1,1) process with $\\phi = .9, \\theta = .5$, and $ \\sigma^2 = 1$.\n",
    "\n",
    "Find the MLEs of the three parameters in\n",
    "each case and compare the estimators to the true values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob 3.10\n",
    "Let $x_t$ represent the cardiovascular mortality series (cmort) discussed in\n",
    "Chapter 2, Example 2.2.\n",
    "\n",
    "(a) Fit an AR(2) to $x_t$ using linear regression as in Example 3.17.\n",
    "\n",
    "(b) Assuming the fitted model in (a) is the true model, find the forecasts over\n",
    "a four-week horizon, $x_{n+m}^n$ , for $m = 1, 2, 3, 4$, and the corresponding 95%\n",
    "prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Reference, Example 2.2\n",
    "#### Pollution, Temperature and Mortality\n",
    "The data shown in Fig. 2.2 are extracted series from a study by Shumway et al. \n",
    "of the possible effects of temperature and pollution on weekly mortality in Los\n",
    "Angeles County. \n",
    "\n",
    "Note the strong seasonal components in all of the series, corresponding to winter-summer variations and the downward trend in the cardiovascular mortality over the 10-year period.\n",
    "\n",
    "A scatterplot matrix, shown in Fig. 2.3, indicates a possible linear relation\n",
    "between mortality and the pollutant particulates and a possible relation to temperature. \n",
    "\n",
    "Note the curvilinear shape of the temperature mortality curve, indicating that\n",
    "higher temperatures as well as lower temperatures are associated with increases in cardiovascular mortality. Based on the scatterplot matrix, we entertain, tentatively, four models where $M_t$ denotes cardiovascular mortality, $T_t$ denotes temperature and $P_t$ denotes the\n",
    "particulate levels.\n",
    "\n",
    "They are\n",
    "\n",
    "$M_t = \\beta_0 + \\beta_1 t + w_t$\n",
    "\n",
    "$M_t = \\beta_0 + \\beta_1 t + \\beta_2 (T_t − T_·) + w_t$\n",
    "\n",
    "$M t = \\beta_0 + \\beta_1 t + \\beta_2 (T_t − T_· ) + \\beta_3 (T_t − T_· ) 2 + w_t $\n",
    "\n",
    "$M t = \\beta_0 + \\beta_1 t + \\beta_2 (T_t − T_· ) + \\beta_3 (T_t − T_· ) 2 + \\beta_4 P_t + w t$\n",
    "\n",
    "where we adjust temperature for its mean, $T_· = 74.26$, to avoid collinearity prob-\n",
    "lems. \n",
    "\n",
    "It is clear that (2.18) is a trend only model, (2.19) is linear temperature, (2.20)\n",
    "is curvilinear temperature and (2.21) is curvilinear temperature and pollution. \n",
    "\n",
    "We summarize some of the statistics given for this particular case in Table 2.2.\n",
    "\n",
    "We note that each model does substantially better than the one before it and that\n",
    "the model including temperature, temperature squared, and particulates does the\n",
    "best, accounting for some 60% of the variability and with the best value for AIC\n",
    "and BIC (because of the large sample size, AIC and AICc are nearly the same).\n",
    "\n",
    "Note that one can compare any two models using the residual sums of squares\n",
    "and (2.11). \n",
    "\n",
    "Hence, a model with only trend could be compared to the full model, \n",
    "$H_0$ : $\\beta_2 = \\beta_3 = \\beta_4 = 0$, using $q = 4, r = 1, n = 508$, and $F_{3,503} = \\frac{(40020 − 20508)/3}{20508/503} = 160$ which exceeds $F_{3,503}(.001) = 5.51$. We obtain the best prediction model. \n",
    "\n",
    "$\\hat{M_t} = 2831.5 − 1.396_{(.10)} t − .472 _{(.032)} (T_t − 74.26) + .023_{(.003)} (T_t − 74.26)^2 + .255_{(.019)} P_t$ , for mortality, where the standard errors, computed from (2.6)–(2.8), are given in\n",
    "parentheses. As expected, a negative trend is present in time as well as a negative\n",
    "coefficient for adjusted temperature.\n",
    "\n",
    "The quadratic effect of temperature can clearly be seen in the scatterplots of Fig. 2.3.\n",
    "\n",
    "Pollution weights positively and can be\n",
    "interpreted as the incremental contribution to daily deaths per unit of particulate\n",
    "pollution.\n",
    "\n",
    "It would still be essential to check the residuals $\\hat{w_t} = M_t − \\hat{M_t}$ for\n",
    "autocorrelation (of which there is a substantial amount), but we defer this question to Sect. 3.8 when we discuss regression with correlated errors.\n",
    "\n",
    "Below is the R code to plot the series, display the scatterplot matrix, fit the final regression model (2.21), and compute the corresponding values of AIC, AICc and\n",
    "BIC. \n",
    "\n",
    "Finally, the use of na.action in lm() is to retain the time series attributes for\n",
    "the residuals and fitted values.\n",
    "\n",
    "```R\n",
    "par(mfrow=c(3,1)) # plot the data\n",
    "plot(cmort, main=\"Cardiovascular Mortality\", xlab=\"\", ylab=\"\")\n",
    "plot(tempr, main=\"Temperature\", xlab=\"\", ylab=\"\")\n",
    "plot(part, main=\"Particulates\", xlab=\"\", ylab=\"\")\n",
    "dev.new()\n",
    "# open a new graphic device\n",
    "ts.plot(cmort,tempr,part, col=1:3) # all on same plot (not shown)\n",
    "dev.new()\n",
    "pairs(cbind(Mortality=cmort, Temperature=tempr, Particulates=part))\n",
    "temp = tempr-mean(tempr) # center temperature\n",
    "temp2 = temp^2\n",
    "# time\n",
    "trend = time(cmort)\n",
    "fit\n",
    "= lm(cmort~ trend + temp + temp2 + part, na.action=NULL)\n",
    "summary(fit)\n",
    "# regression results\n",
    "summary(aov(fit))\n",
    "# ANOVA table\n",
    "(compare to next line)\n",
    "summary(aov(lm(cmort~cbind(trend, temp, temp2, part)))) # Table 2.1\n",
    "num = length(cmort)\n",
    "# sample size\n",
    "AIC(fit)/num - log(2*pi) # AIC\n",
    "BIC(fit)/num - log(2*pi) # BIC\n",
    "(AICc = log(sum(resid(fit)^2)/num) + (num+5)/(num-5-2)) # AICc\n",
    "```\n",
    "\n",
    "As previously mentioned, it is possible to include lagged variables in time series\n",
    "regression models and we will continue to discuss this type of problem throughout\n",
    "the text. This concept is explored further in Problem 2.2 and Problem 2.10. The\n",
    "following is a simple example of lagged regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3.17 The PACF of an Invertible MA(q)\n",
    "\n",
    "For an invertible MA(q), we can write $x_t = -\\sum_j=1^\\infty \\pi_j x_{t−j} + w_t$ . \n",
    "\n",
    "Moreover, no finite representation exists.\n",
    "\n",
    "From this result, it should be apparent that the PACF will never cut off, as in the case of an AR(p).\n",
    "\n",
    "For an MA(1), $x_t = w_t + \\theta w_{t−1}$ , with $|\\theta| < 1$, calculations similar to Example 3.15 will yield $\\phi_{22} = -\\theta^2 / (1 + \\theta^2 + \\theta^4 ).\n",
    "\n",
    "For the MA(1) in general, we can show\n",
    "that \n",
    "\n",
    "$\\phi_{hh} = \\frac{(-\\theta)^h (1-\\theta^2)}{1 - \\theta^{2(h+1)}} , h\\geq 1$\n",
    "\n",
    "In the next section, we will discuss methods of calculating the PACF. The PACF\n",
    "for MA models behaves much like the ACF for AR models. Also, the PACF for AR\n",
    "models behaves much like the ACF for MA models. Because an invertible ARMA\n",
    "model has an infinite AR representation, the PACF will not cut off. We may summarize\n",
    "\n",
    "these results in Table 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob 3.33\n",
    "3.33 Fit an ARIMA(p, d, q) model to the global temperature data gtemp per-\n",
    "forming all of the necessary diagnostics. After deciding on an appropriate\n",
    "model, forecast (with limits) the next 10 years. Comment.\n",
    "# Prob 3.42\n",
    "3.42 Consider the series x t = $w_t$ −w t−1 , where $w_t$ is a white noise process with\n",
    "2\n",
    ". Suppose we consider the problem of predicting\n",
    "mean zero and variance \\sigma w\n",
    "x n+1 , based on only x 1 , . . . , x n . Use the Projection theorem to answer the\n",
    "questions below.\n",
    "(a) Show the best linear predictor is\n",
    "n\n",
    "x nn+1 = −\n",
    "1 X\n",
    "k x k .\n",
    "n +1\n",
    "k=1\n",
    "(b) Prove the mean square error is\n",
    "E(x n+1 − x nn+1 ) 2 =\n",
    "n +2 2\n",
    "\\sigma .\n",
    "n +1\n",
    "\n",
    "# Prob 4.1\n",
    "4.1 Verify that for any positive integer n and j, k = 0, 1, . . . , [[n/2]], where [[·]] denotes\n",
    "the greatest integer function:\n",
    "(a) Except for j = 0 or j = n/2,12\n",
    "n\n",
    "\u0004\n",
    "cos 2 (2\\pit j/n) =\n",
    "t=1\n",
    "n\n",
    "\u0004\n",
    "sin 2 (2\\pit j/n) = n/2.\n",
    "t=1\n",
    "(b) When j = 0 or j = n/2,\n",
    "n\n",
    "\u0004\n",
    "cos 2 (2\\pit j/n) = n but\n",
    "t=1\n",
    "(c) For j $ k,\n",
    "n\n",
    "\u0004\n",
    "n\n",
    "\u0004\n",
    "sin 2 (2\\pit j/n) = 0.\n",
    "t=1\n",
    "cos(2\\pit j/n) cos(2\\pitk/n) =\n",
    "t=1\n",
    "n\n",
    "\u0004\n",
    "sin(2\\pit j/n) sin(2\\pitk/n) = 0.\n",
    "t=1\n",
    "Also, for any j and k,\n",
    "n\n",
    "\u0004\n",
    "cos(2\\pit j/n) sin(2\\pitk/n) = 0.) Inspecting the series\n",
    "shows a smoother version of the first series, reflecting the fact that the slower\n",
    "oscillations are more apparent and some of the faster oscillations are taken out. We\n",
    "begin to notice a similarity to the SOI in Fig. 1.5, or perhaps, to some of the fMRI\n",
    "series in Fig. 1.6.1.2 Time Series Statistical Models\n",
    "11\n",
    "A linear combination of values in a time series such as in (1.1) is referred to,\n",
    "generically, as a filtered series; hence the command filter in the following code\n",
    "for Fig. 1.8.\n",
    "```\n",
    "w = rnorm(500,0,1)\n",
    "# 500 N(0,1) variates\n",
    "v = filter(w, sides=2, filter=rep(1/3,3)) # moving average\n",
    "par(mfrow=c(2,1))\n",
    "plot.ts(w, main=\"white noise\")\n",
    "plot.ts(v, ylim=c(-3,3), main=\"moving average\")\n",
    "```\n",
    "The speech series in Fig. 1.3 and the Recruitment series in Fig. 1.5, as well as\n",
    "some of the MRI series in Fig. 1.6, differ from the moving average series because one\n",
    "particular kind of oscillatory behavior seems to predominate, producing a sinusoidal\n",
    "type of behavior. A number of methods exist for generating series with this quasi-\n",
    "periodic behavior; we illustrate a popular one based on the autoregressive model\n",
    "considered in Chap. 3.\n",
    "\n",
    "## Example 1.20 Stationarity of a Moving Average\n",
    "The three-point moving average process of Example 1.9 is stationary because,\n",
    "from Example 1.13 and Example 1.17, the mean and autocovariance functions\n",
    "$μ_vt = 0$, and\n",
    "3 2\n",
    "⎧\n",
    "⎪\n",
    "⎪ 9 σ w h = 0,\n",
    "⎪\n",
    "⎪\n",
    "⎨ 2 σ 2 h = ±1,\n",
    "⎪\n",
    "γ v (h) = 9 1 w 2\n",
    "⎪ 9 σ w h = ±2,\n",
    "⎪\n",
    "⎪\n",
    "⎪\n",
    "⎪ 0\n",
    "|h| > 2\n",
    "⎩\n",
    "are independent of time t, satisfying the conditions of Definition 1.7.\n",
    "The autocorrelation function is given by\n",
    "\\rho v (h) =\n",
    "⎧\n",
    "1\n",
    "⎪\n",
    "⎪\n",
    "⎪\n",
    "⎪\n",
    "⎨ 2\n",
    "⎪\n",
    "3\n",
    "1\n",
    "⎪\n",
    "⎪\n",
    "3\n",
    "⎪\n",
    "⎪\n",
    "⎪ 0\n",
    "⎩\n",
    "h = 0,\n",
    "h = ±1,\n",
    "h = ±2,\n",
    "|h| > 2.\n",
    "Figure 1.12 shows a plot of the autocorrelations as a function of lag h. Note that\n",
    "the ACF is symmetric about lag zero.\n",
    "\n",
    "# Prob 2.3\n",
    "2.3 Repeat the following exercise six times and then discuss the results. Gen-\n",
    "erate a random walk with drift, (1.4), of length n = 100 with \\delta = .01 and\n",
    "\\sigma w = 1. Call the data x t for t = 1, . . . , 100. Fit the regression x t = \\betat + w t\n",
    "using least squares. Plot the data, the mean function (i.e., \\mu t = .01 t) and the\n",
    "fitted line, x\n",
    "b t = \\beta b t, on the same graph. Discuss your results.\n",
    "the following R code may be useful:\n",
    "\n",
    "```R\n",
    "par(mfcol = c(3,2)) # set up graphics\n",
    "for (i in 1:6){\n",
    "x = ts(cumsum(rnorm(100,.01,1)))\n",
    "# the data\n",
    "reg = lm(x~0+time(x), na.action=NULL) # the regression\n",
    "plot(x) # plot data\n",
    "lines(.01*time(x), col=\"red\", lty=\"dashed\") # plot mean\n",
    "abline(reg, col=\"blue\") } # plot regression line\n",
    "```\n",
    "# Prob 2.11\n",
    "2.11 Consider the two weekly time series oil and gas. the oil series is in\n",
    "dollars per barrel, while the gas series is in cents per gallon; see Appendix R\n",
    "for details.\n",
    "(a) Plot the data on the same graph. Which of the simulated series displayed in\n",
    "§1.3 do these series most resemble? Do you believe the series are stationary\n",
    "(explain your answer)?\n",
    "(b) In economics, it is often the percentage change in price (termed growth rate\n",
    "or return), rather than the absolute price change, that is important. Argue\n",
    "that a transformation of the form y t = \\nabla log x t might be applied to the\n",
    "data, where x t is the oil or gas price series [see the hint in Problem 2.8(d)].\n",
    "(c) transform the data as described in part (b), plot the data on the same\n",
    "graph, look at the sample ACFs of the transformed data, and comment.\n",
    "[Hint: poil = diff(log(oil)) and pgas = diff(log(gas)).]\n",
    "(d) Plot the CCF of the transformed data and comment the small, but signif-\n",
    "icant values when gas leads oil might be considered as feedback. [Hint:\n",
    "ccf(poil, pgas) will have poil leading for negative lag values.]\n",
    "(e) Exhibit scatterplots of the oil and gas growth rate series for up to three\n",
    "weeks of lead time of oil prices; include a nonparametric smoother in each\n",
    "plot and comment on the results (e.g., Are there outliers? Are the rela-\n",
    "tionships linear?). [Hint: lag.plot2(poil, pgas, 3).]\n",
    "(f) there have been a number of studies questioning whether gasoline prices\n",
    "respond more quickly when oil prices are rising than when oil prices are\n",
    "falling (“asymmetry”). We will attempt to explore this question here with\n",
    "simple lagged regression; we will ignore some obvious problems such as\n",
    "outliers and autocorrelated errors, so this will not be a definitive analysis.\n",
    "Let G t and O t denote the gas and oil growth rates.\n",
    "(i) Fit the regression (and comment on the results)\n",
    "G t = \\alpha 1 + \\alpha 2 I t + \\beta 1 O t + \\beta 2 O t−1 + $w_t$ ,\n",
    "where I t = 1 if O t ≥ 0 and 0 otherwise (I t is the indicator of no\n",
    "growth or positive growth in oil price). Hint:\n",
    "1\n",
    "2\n",
    "3\n",
    "indi = ifelse(poil < 0, 0, 1)\n",
    "mess = ts.intersect(pgas, poil, poilL = lag(poil,-1), indi)\n",
    "summary(fit <- lm(pgas~ poil + poilL + indi, data=mess))\n",
    "(ii) What is the fitted model when there is negative growth in oil price at\n",
    "time t? What is the fitted model when there is no or positive growth\n",
    "in oil price? Do these results support the asymmetry hypothesis?\n",
    "(iii) Analyze the residuals from the fit and comment.\n",
    "# Prob 3.6\n",
    "3.6 For the AR(2) model given by x t = −.9x t−2 + $w_t$ , find the roots of the\n",
    "autoregressive polynomial, and then sketch the ACF, \\rho(h).\n",
    "# Prob 3.9\n",
    "3.9 Generate n = 100 observations from each of the three models discussed in\n",
    "Problem 3.8. Compute the sample ACF for each model and compare it to the\n",
    "theoretical values. Compute the sample PACF for each of the generated series\n",
    "and compare the sample ACFs and PACFs with the general results given in\n",
    "table 3.1.\n",
    "Section 3.5\n",
    "# Prob 3.21\n",
    "3.21 Generate 10 realizations of length n = 200 each of an ARMA(1,1) process\n",
    "with \\phi = .9, θ = .5 and \\sigma 2 = 1. Find the MLEs of the three parameters in\n",
    "each case and compare the estimators to the true values.\n",
    "# Prob 3.10\n",
    "3.10 Let x t represent the cardiovascular mortality series (cmort) discussed in\n",
    "Chapter 2, Example 2.2.\n",
    "(a) Fit an AR(2) to x t using linear regression as in Example 3.17.\n",
    "(b) Assuming the fitted model in (a) is the true model, find the forecasts over\n",
    "a four-week horizon, x nn+m , for m = 1, 2, 3, 4, and the corresponding 95%\n",
    "prediction intervals.\n",
    "\n",
    "## Example 2.2\n",
    "Example 2.2 Pollution, Temperature and Mortality\n",
    "The data shown in Fig. 2.2 are extracted series from a study by Shumway et al. [183]\n",
    "of the possible effects of temperature and pollution on weekly mortality in Los\n",
    "Angeles County. Note the strong seasonal components in all of the series, corre-\n",
    "sponding to winter-summer variations and the downward trend in the cardiovascular\n",
    "mortality over the 10-year period.\n",
    "A scatterplot matrix, shown in Fig. 2.3, indicates a possible linear relation\n",
    "between mortality and the pollutant particulates and a possible relation to tempera-\n",
    "ture. Note the curvilinear shape of the temperature mortality curve, indicating that\n",
    "higher temperatures as well as lower temperatures are associated with increases in\n",
    "cardiovascular mortality.\n",
    "Based on the scatterplot matrix, we entertain, tentatively, four models where\n",
    "M t denotes cardiovascular mortality, T t denotes temperature and P t denotes the\n",
    "particulate levels. They are\n",
    "M t = β 0 + β 1 t + $w_t$ (2.18)\n",
    "M t = β 0 + β 1 t + β 2 (T t − T · ) + w t\n",
    "M t = β 0 + β 1 t + β 2 (T t − T · ) + β 3 (T t − T · ) 2 + $w_t$ (2.19)\n",
    "(2.20)\n",
    "M t = β 0 + β 1 t + β 2 (T t − T · ) + β 3 (T t − T · ) 2 + β 4 P t + w t\n",
    "(2.21)\n",
    "where we adjust temperature for its mean, T · = 74.26, to avoid collinearity prob-\n",
    "lems. It is clear that (2.18) is a trend only model, (2.19) is linear temperature, (2.20)\n",
    "\n",
    "is curvilinear temperature and (2.21) is curvilinear temperature and pollution. We\n",
    "summarize some of the statistics given for this particular case in Table 2.2.\n",
    "We note that each model does substantially better than the one before it and that\n",
    "the model including temperature, temperature squared, and particulates does the\n",
    "best, accounting for some 60% of the variability and with the best value for AIC\n",
    "and BIC (because of the large sample size, AIC and AICc are nearly the same).\n",
    "Note that one can compare any two models using the residual sums of squares\n",
    "and (2.11). Hence, a model with only trend could be compared to the full model,\n",
    "H 0 : β 2 = β 3 = β 4 = 0, using q = 4, r = 1, n = 508, and (40, 020 − 20, 508)/3\n",
    "= 160,\n",
    "20, 508/503\n",
    "which exceeds F 3,503 (.001) = 5.51. We obtain the best prediction model,\n",
    "F 3,503 =\n",
    "M̂ t = 2831.5 − 1.396 (.10) t − .472 (.032) (T t − 74.26)\n",
    "+ .023 (.003) (T t − 74.26) 2 + .255 (.019) P t ,\n",
    "for mortality, where the standard errors, computed from (2.6)–(2.8), are given in\n",
    "parentheses. As expected, a negative trend is present in time as well as a negative\n",
    "coefficient for adjusted temperature. The quadratic effect of temperature can clearly\n",
    "be seen in the scatterplots of Fig. 2.3. Pollution weights positively and can be\n",
    "interpreted as the incremental contribution to daily deaths per unit of particulate\n",
    "pollution. It would still be essential to check the residuals ŵ t = M t − M̂ t for\n",
    "autocorrelation (of which there is a substantial amount), but we defer this question\n",
    "to Sect. 3.8 when we discuss regression with correlated errors.\n",
    "Below is the R code to plot the series, display the scatterplot matrix, fit the final\n",
    "regression model (2.21), and compute the corresponding values of AIC, AICc and\n",
    "BIC.2 Finally, the use of na.action in lm() is to retain the time series attributes for\n",
    "the residuals and fitted values.\n",
    "\n",
    "```R\n",
    "par(mfrow=c(3,1)) # plot the data\n",
    "plot(cmort, main=\"Cardiovascular Mortality\", xlab=\"\", ylab=\"\")\n",
    "plot(tempr, main=\"Temperature\", xlab=\"\", ylab=\"\")\n",
    "plot(part, main=\"Particulates\", xlab=\"\", ylab=\"\")\n",
    "dev.new()\n",
    "# open a new graphic device\n",
    "ts.plot(cmort,tempr,part, col=1:3) # all on same plot (not shown)\n",
    "dev.new()\n",
    "pairs(cbind(Mortality=cmort, Temperature=tempr, Particulates=part))\n",
    "temp = tempr-mean(tempr) # center temperature\n",
    "temp2 = temp^2\n",
    "# time\n",
    "trend = time(cmort)\n",
    "fit\n",
    "= lm(cmort~ trend + temp + temp2 + part, na.action=NULL)\n",
    "summary(fit)\n",
    "# regression results\n",
    "summary(aov(fit))\n",
    "# ANOVA table\n",
    "(compare to next line)\n",
    "summary(aov(lm(cmort~cbind(trend, temp, temp2, part)))) # Table 2.1\n",
    "num = length(cmort)\n",
    "# sample size\n",
    "AIC(fit)/num - log(2*pi) # AIC\n",
    "BIC(fit)/num - log(2*pi) # BIC\n",
    "(AICc = log(sum(resid(fit)^2)/num) + (num+5)/(num-5-2)) # AICc\n",
    "```\n",
    "\n",
    "As previously mentioned, it is possible to include lagged variables in time series\n",
    "regression models and we will continue to discuss this type of problem throughout\n",
    "the text. This concept is explored further in Problem 2.2 and Problem 2.10. The\n",
    "following is a simple example of lagged regression.\n",
    "\n",
    "## Example 3.17 The PACF of an Invertible MA(q)\n",
    "\u0011\n",
    "For an invertible MA(q), we can write x t = − ∞\n",
    "j=1 π j x t−j + $w_t$ . Moreover, no finite\n",
    "representation exists. From this result, it should be apparent that the PACF will\n",
    "never cut off, as in the case of an AR(p).\n",
    "For an MA(1), x t = $w_t$ + θw t−1 , with |θ| < 1, calculations similar to Exam-\n",
    "ple 3.15 will yield φ 22 = −θ 2 /(1 + θ 2 + θ 4 ). For the MA(1) in general, we can show\n",
    "that\n",
    "(−θ) h (1 − θ 2 )\n",
    ", h ≥ 1.\n",
    "φ hh = −\n",
    "1 − θ 2(h+1)\n",
    "In the next section, we will discuss methods of calculating the PACF. The PACF\n",
    "for MA models behaves much like the ACF for AR models. Also, the PACF for AR\n",
    "models behaves much like the ACF for MA models. Because an invertible ARMA\n",
    "model has an infinite AR representation, the PACF will not cut off. We may summarize\n",
    "these results in Table 3.1.\n",
    "\n",
    "\n",
    "# Prob 3.33\n",
    "3.33 Fit an ARIMA(p, d, q) model to the global temperature data gtemp per-\n",
    "forming all of the necessary diagnostics. After deciding on an appropriate\n",
    "model, forecast (with limits) the next 10 years. Comment.\n",
    "# Prob 3.42\n",
    "3.42 Consider the series x t = $w_t$ −w t−1 , where $w_t$ is a white noise process with\n",
    "2\n",
    ". Suppose we consider the problem of predicting\n",
    "mean zero and variance \\sigma w\n",
    "x n+1 , based on only x 1 , . . . , x n . Use the Projection theorem to answer the\n",
    "questions below.\n",
    "(a) Show the best linear predictor is\n",
    "n\n",
    "x nn+1 = −\n",
    "1 X\n",
    "k x k .\n",
    "n +1\n",
    "k=1\n",
    "(b) Prove the mean square error is\n",
    "E(x n+1 − x nn+1 ) 2 =\n",
    "n +2 2\n",
    "\\sigma .\n",
    "n +1\n",
    "\n",
    "# Prob 4.1\n",
    "4.1 Verify that for any positive integer n and j, k = 0, 1, . . . , [[n/2]], where [[·]] denotes\n",
    "the greatest integer function:\n",
    "(a) Except for j = 0 or j = n/2,12\n",
    "n\n",
    "\u0004\n",
    "cos 2 (2\\pit j/n) =\n",
    "t=1\n",
    "n\n",
    "\u0004\n",
    "sin 2 (2\\pit j/n) = n/2.\n",
    "t=1\n",
    "(b) When j = 0 or j = n/2,\n",
    "n\n",
    "\u0004\n",
    "cos 2 (2\\pit j/n) = n but\n",
    "t=1\n",
    "(c) For j $ k,\n",
    "n\n",
    "\u0004\n",
    "n\n",
    "\u0004\n",
    "sin 2 (2\\pit j/n) = 0.\n",
    "t=1\n",
    "cos(2\\pit j/n) cos(2\\pitk/n) =\n",
    "t=1\n",
    "n\n",
    "\u0004\n",
    "sin(2\\pit j/n) sin(2\\pitk/n) = 0.\n",
    "t=1\n",
    "Also, for any j and k,\n",
    "n\n",
    "\u0004\n",
    "cos(2\\pit j/n) sin(2\\pitk/n) = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
